Hello, I am Danjie Tang. I am going to become a third year computer engineering student at university of toronto. I've been applying to a lot of job posts recently, and unfortunately there haven't been much replies. That being said, I did found an interesting research position with Professor Sanner for the summer. I initially thought that it would be some kind of really interesting machine learning project, however as it had turn out it is more of a software engineering project than a machine learning one. It is true that we have to use some machine learning knowledge, for example embedding. But for the most of the time it had been software engineering. Recently, I purchase a new laptop for the purpose of doing machine learning training. Thanks to its 4060 GPU, I can do machine learning with GPU with quite a big batch size. Last week had been an interesting week, I met up with my old friends from high school and played some really interesting card board games. But that's not it, after that, I met with some of my good old friends from first year university. Gosh, those were some good time. We went to KTV together. Yesterday, I came up with a brilliant idea for the research. We were looking at information retrieval for information systems with reviews for each item. Our current method is using late fusion to find the most relevant restaurant given its review information. Basically, we use some transformers to embed all the reviews for all the restaurants, and then we use the same embedding method to embed user input. Say for example, user asks for a restaurants that has air conditioning and free wifi. We are going to embed that, and find the similarity score between that query embedding and all the restaurant reviews. By similarity score, I meant dot product. Late fusion takes the top k most similar review for each restaurant and find the average of those similarity scores. The average is recorded down as the restaurant similarity score. We then take the top k restaurant similarity scores and find the most similar restaurants. The flaw of this method is that we are actually throwing away lots of useful information by only looking at the top k reviews to find restaurant similarity score. Instead, I proposed to use attention mechanism to find the attention score for each of the reviews and pass all the similarity for each restaurant through a softmax function. Using the output of the softmax function as attention score, we easily find the restaurant embedding by multiplying each review and its attention score. After that we can do another similarity test for all the restaurant embeddings and therefore find the most similar restaurant. From my current understanding of deep learning, I believe this method is worth a shot. My professor agreed that this is an interesting proposal. However, in his reply, he implied that we should be focusing on our current task at the hand instead of doing something new. Honestly, I might as well go and try out this new method on my own free time. I am creating my own data set of how I normally talk. I went to high school in Markham, a beautiful city to the north of Toronto. I spent 4 years in Markham, thinking back, I don't regret a single day I spent in that high school. I made some excellent friends there in Markham. However, I do have one tiny regret. I should've spend more time on programming back in highschool. Anyway, let's talk about my trip on deep learning. I have began learning deep learning in January 2023, however, I couldn't find where to learn. I was learning on Coursera, but the courses there were so short. They literally spent like 12 minutes on RNN, how is anyone suppose to learn? Fortunately, I know someone in upper grade that has the recording of APS360 from last year. So I learned CNN and basic feed forward neural networks on my own time. It was only until this summer, when I really started having time for myself and actually starting to make progress on deep learning. In fact, I am creating a dataset for my potential project to create a large language model that sounds exactly like me. But before that, I am going to create a Trump Tweet Identifier, that can basically identify whether a sentence is said by Trump or not. That's why I am typing these garbage notes down. Simply trying to create a dataset of how I normally speak. Gosh, I don't even know what to say now. My passport is expiring soon, so I went to the embassy yesterday. However, I was refused to enter because they close at 12 pm, that's noon. So I had to go again today. Today, when I arrived there. Turns out that they are not accepting any in person renewal of passport, because of the covid-19 pandemic. What a bunch of idiots. It's May 31, 2023. The world health organization had already declared the pandemic to be gone, yet they still force us to wear masks when inside. I really don't know what else can I talk about. Let's talk about airplanes. Back in high school, during a conversation with one of my friends, I realized that I have a strong passion towards aviation. Like I've always thought airplanes are so cool, but I've actually never had a deep thought about it. So, I download an airplane flight simulator called Infinite Flight. Looking back, the flight simulator is kinda bad in quality. But I fell in love with flying an aircraft after that. I used to play flight simulator every day. Gosh, those were the simple days. It was about sometime in the summer of 2020, when covid-19 was at its peak in terms of impact to our life. Schools were locked down and I had pretty much nothing to do all day along. Looking back, I wish I had spent more time on deep learning. But honestly, the time during lockdown and social distance contains most of my favourite memory from high school. So, I soon started watching youtube videos about it and learning about how many models there are. Right now, I can tell 80% of the time when I walk into an airport which airplane is which. I can't tell the sepecific details but I can which plane is which model from which generation most of the time. For example: I can tell a 737 Next generation and a 737 max. But I can't tell if it is a 737-800 or a 737-900. Of course, I can tell the difference between a 737 and 747 and so on. Because they are different models. I like to go to parks, they give me a sense of calmness and peace. Back in high school, I used to spend around half an hour in park every day. I am not joking when I say that my favouite place in the world is the park right next to where I used to live. I used to bring friends to there and we'll sit on a bench and just chat. Gosh, I miss those good old days. In universities you really don't get the chance to stop for a moment and take a deep breath. Of course the park in Markham is too far for me now. So instead, I like to go to Queens park. It is also a pretty big park but sometimes there's people smoking. Speaking of smoking, I hate people when they smoke on the street. Like why would you smoke near an intersection where there's a bunch of people right next to you! There's just too many people smoking in Toronto and I hate it. It is part of the reason why I prefer suburban than downtown. The other part is traffic. Anyway, back to the park. Queens park is an awesome park, I really enjoy it. But its just not as good as Swan Lake park in Markham. Oh right, did I mention that I have a cat. His name is little orange. He was borned in December last year, although the original owner doesn't remember what's the exact date. He was born in a farm in Saskatchewan, however, his original owner decided to give him away because he was too friendly. And honest they were right. This cat is literally the most friendly creature I've ever seen. You know for other cats, when you put your feet above their head. Usually they will escape right. This cat won't. There is no defence mechanism in its mind like at all. The original owner said if little orange stay in the farm, there's a good chance that it will get eaten. That's why they decided to give it to us. Man, I really don't know what else can I talk about. Today is the second day of me creating my own dataset of my random tweets. It's been a pretty boring day so far, nothing interesting had happened yet. My research is going just fine. Today, I got assigned the new task of creating an evaluation for our current information retrieval system. It is actually pretty interesting, because after I finish evaluationg our current information retrieval system. I can do it again on the attention mechanism information retrieval system. If, and it is a big if, I can prove using evaluation that my attention mechanism based information retrieval system can out perform our current information retrieval system. We can potential publish a paper on that. It'll be like the first step towards publishing a paper. Let's talk about operating system. I don't like Windows. Actually, to be more accurate I don't like Microsoft. Like they have excellent products, like say for example bing using ChatGPT, Microsoft flight simulator, and most importantly Windows operating system. But the terminal of Windows sucks. You can't do ls, you have to do dir. And the file extension  for windows is really confusing. I have a strong perference for MacOS and Linux over windows. Unfortunately, I have to deal with it if I want to use a device with a GPU. Actually, if you think about it, I can also use Linux. But Linux has a relatively weaker compatibility comparing to MacOS and Windows. Let's talk about different universities in Canada. There are a bunch of universities in Canada, and I applied to around 13 universities. Let's begin with University of Toronto, my home school. University of Toronto locates in downtown Toronto. It is one of the best universities in Canada. In fact, if you look at its ranking it is the best university in Canada. It is ranked to be somewhat around 20th best universities, depending on which ranking you are looking at. So I guess I should be kinda proud? Even though it doesn't feel that way. It feels like University of Toronto is just some random universities in the world. It was only recently that I've learned all the amazing achievements University of Toronto had done in the field of machine learning. One of the founder of the original transformer paper is from University of Toronto, and Jeffery Hinton is from University of Toronto. The only flaw I see in the University of Toronto is that it is located in the very downtown of Toronto. I don't like downtown. I prefer suburban. Sometimes I wonder why I didn't went to University of Waterloo. Lol, but they rejected my computer science application. Kinda sad. Honestly, if they had accepted my computer science application, I probably would've be in University of Waterloo now. McGill and University of British Columbia are fine. Like they are not the top tier, but they are not bad. I've actually been to University of British Columbia once. It was astonishingly beautiful. I want to go to United States for grad school. University of Toronto is pretty much the all there is for Canada. If I am going to the United States for grad school, I would like it to have at least a higher ranking than University of Toronto. That's kind of hard if I am being honest, because University of Toronto is already a high ranking school. Especially in the field of computer science. Let's keep talking. Today is an excellent day, I was assigned the task of doing information retrieval system evaluation yesterday, and I had no idea what to do. So I had a meeting with the grad students who already did evaluation on their information retrieval system. Turns out that it wasn't really helpful. So I was left there confused for a long time, and I had to look deep into their code for what they were doing. Thanks to ChatGPT, I can do that relatively easily. Gosh, ChatGPT is good. Without it I would've suffered so much in this internship. I do wonder when will the GitHub Copilot X come out. I heard it is supported using GPT-4 so I am getting kinda excited for it. There is actually one very good news today, so you know I had been trying to create my own vanilla transformer right. Yesterday the loss won't come down. It stays at a binary cross entropy loss of 0.693, which is equivalent of always outputing 0.5 as the output. It got me super confused, and I didn't know how to fix it. Turns out that today, when I try to use layer normalization instead of batch normalization. It works! This is so great. Now, I am going to try to train my model using this file and see if it can tell the difference between me talking and trump talking. 