{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvBqu1zyKOqR"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install nltk\n",
    "!pip install matplotlib\n",
    "!pip install torch\n",
    "!pip install ipywidgets\n",
    "!pip install huggingface_hub\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seN3tFvRFjeX"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import notebook_login\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from torch.cuda import amp\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTzjE8UgnuXV"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_token = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "epochs = 100\n",
    "batch_size = 400\n",
    "validation_batch_size = 1\n",
    "weight_decay = 1e-3\n",
    "drop_out_rate = 0.5\n",
    "lr = 1e-3\n",
    "gamma = 0.8\n",
    "num_layer = 4\n",
    "gradient_accumulation_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_hpclrXQVxG"
   },
   "source": [
    "# Load llama2 and use its tokenizer and word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download llama2 and its tokenizer\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# All word embedding\n",
    "embedding_layer = model.get_input_embeddings()\n",
    "\n",
    "# Delete gpt2 becase we are no longer using it.\n",
    "del model\n",
    "\n",
    "# Store how many embedding and embedding dimension of our llama2\n",
    "num_embeddings, embedding_dim = embedding_layer.weight.size()\n",
    "\n",
    "# Create a padding embedding and initialize the padding embedding with zeros\n",
    "padding_embedding = torch.nn.Embedding(1, embedding_dim)\n",
    "padding_embedding.weight.data.zero_()\n",
    "num_embeddings += 1\n",
    "\n",
    "# Concatenate the new padding embedding with the existing word embeddings\n",
    "word_embeddings_tensor = torch.cat([embedding_layer.weight, padding_embedding.weight], 0)\n",
    "# word_embeddings_tensor = word_embeddings_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKae-ChZQTH6"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttQJS0ZaloO8"
   },
   "outputs": [],
   "source": [
    "# Run this code if you want to do data preprocessing\n",
    "tokenized_data = []\n",
    "attention_data = []\n",
    "\n",
    "# Download dataset\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\")\n",
    "training_dataset = dataset[\"train\"]\n",
    "\n",
    "# Tokenize all training data and filter those longer than token limit\n",
    "for i in tqdm(range(len(training_dataset))):\n",
    "    text = training_dataset[i][\"text\"]\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentences = [sentence+\"</s>\" for sentence in sentences]\n",
    "\n",
    "    # Tokenize input\n",
    "    tokenized_sentence = tokenizer(sentences, padding='max_length', max_length=max_token)\n",
    "    input_ids = tokenized_sentence[\"input_ids\"]\n",
    "    attention_mask = tokenized_sentence[\"attention_mask\"]\n",
    "\n",
    "    # Filter those longer than max_token\n",
    "    for j in range(len(input_ids)):\n",
    "        if len(input_ids[j]) <= max_token:\n",
    "            tokenized_data.append(input_ids[j])\n",
    "            attention_data.append(attention_mask[j])\n",
    "\n",
    "# Write into json\n",
    "with open('tokenized_data.json', 'w') as file:\n",
    "    # Write the JSON data\n",
    "    json.dump(tokenized_data, file)\n",
    "\n",
    "with open('attention_data.json', 'w') as file:\n",
    "    # Write the JSON data\n",
    "    json.dump(attention_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from json (If you had already preprocessed\n",
    "with open('tokenized_data.json', 'r') as file:\n",
    "    # Load the data from the file\n",
    "    tokenized_data = json.load(file)\n",
    "\n",
    "with open('attention_data.json', 'r') as file:\n",
    "    # Load the data from the file\n",
    "    attention_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Rk3HQA4wJVk"
   },
   "outputs": [],
   "source": [
    "total_data_num = len(tokenized_data)\n",
    "training_data_num = int(total_data_num * 0.95)\n",
    "\n",
    "training_data = torch.tensor(tokenized_data[:training_data_num])\n",
    "training_attention = torch.tensor(attention_data[:training_data_num])\n",
    "validation_data = torch.tensor(tokenized_data[training_data_num:])\n",
    "validation_attention = torch.tensor(attention_data[training_data_num:])\n",
    "\n",
    "# Create a TensorDataset\n",
    "training_data = TensorDataset(training_data, training_attention)\n",
    "validation_data = TensorDataset(validation_data, validation_attention)\n",
    "\n",
    "# Use DataLoader for batching, etc.\n",
    "training_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=validation_batch_size, shuffle=True)\n",
    "\n",
    "# Free up memory\n",
    "del tokenized_data\n",
    "del attention_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_OH9l-ln-Vt"
   },
   "source": [
    "# Create positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsFa_6UDnpRE"
   },
   "outputs": [],
   "source": [
    "# Positional encoding\n",
    "max_token_pos = max_token - 1\n",
    "pos_matrix = torch.empty(max_token_pos, embedding_dim)\n",
    "for i in range(max_token_pos):\n",
    "    for j in range(0, embedding_dim, 2):\n",
    "        pos_matrix[i, j] = np.sin(i/(10000**(j/embedding_dim)))\n",
    "        if(j+1<embedding_dim):\n",
    "            pos_matrix[i, j+1] = np.cos(i/(10000**(j/embedding_dim)))\n",
    "pos_matrix = pos_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGccxpfWY3OX"
   },
   "source": [
    "# Instantiate LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9IrxjMxY82i"
   },
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return tensor * torch.sigmoid(tensor)\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.V = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.swish = Swish()\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        W = self.W(tensor)\n",
    "        V = self.V(tensor)\n",
    "        return self.swish(W) * V\n",
    "\n",
    "# This is root mean square norm implementation by author\n",
    "# I do not take any credit for this\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, p=-1., eps=1e-8, bias=False):\n",
    "        \"\"\"\n",
    "            Root Mean Square Layer Normalization\n",
    "        :param d: model size\n",
    "        :param p: partial RMSNorm, valid value [0, 1], default -1.0 (disabled)\n",
    "        :param eps:  epsilon value, default 1e-8\n",
    "        :param bias: whether use bias term for RMSNorm, disabled by\n",
    "            default because RMSNorm doesn't enforce re-centering invariance.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.bias = bias\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones(d))\n",
    "        self.register_parameter(\"scale\", self.scale)\n",
    "\n",
    "        if self.bias:\n",
    "            self.offset = nn.Parameter(torch.zeros(d))\n",
    "            self.register_parameter(\"offset\", self.offset)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.p < 0. or self.p > 1.:\n",
    "            norm_x = x.norm(2, dim=-1, keepdim=True)\n",
    "            d_x = self.d\n",
    "        else:\n",
    "            partial_size = int(self.d * self.p)\n",
    "            partial_x, _ = torch.split(x, [partial_size, self.d - partial_size], dim=-1)\n",
    "\n",
    "            norm_x = partial_x.norm(2, dim=-1, keepdim=True)\n",
    "            d_x = partial_size\n",
    "\n",
    "        rms_x = norm_x * d_x ** (-1. / 2)\n",
    "        x_normed = x / (rms_x + self.eps)\n",
    "\n",
    "        if self.bias:\n",
    "            return self.scale * x_normed + self.offset\n",
    "\n",
    "        return self.scale * x_normed\n",
    "\n",
    "class MyLlamaLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, expand_factor: int = 4):\n",
    "        super().__init__()\n",
    "        # Transformer layer\n",
    "        self.rms_norm1 = RMSNorm(embedding_dim)\n",
    "        self.multihead_attention = nn.MultiheadAttention(embedding_dim, num_heads=num_heads)\n",
    "        self.rms_norm2 = RMSNorm(embedding_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim*expand_factor),\n",
    "            RMSNorm(embedding_dim*expand_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_out_rate),\n",
    "            RMSNorm(embedding_dim*expand_factor),\n",
    "            nn.Linear(embedding_dim*expand_factor, embedding_dim),\n",
    "            nn.Dropout(drop_out_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        # Reshape to follow [seq_length, batch_size, embedding_size]\n",
    "        tensor = tensor.transpose(0, 1)\n",
    "\n",
    "        # Actually go through the transformer layer\n",
    "        tensor_skip = tensor\n",
    "        tensor = self.rms_norm1(tensor)\n",
    "        tensor_skip = tensor_skip + self.multihead_attention(tensor, tensor, tensor, attn_mask=mask)[0]\n",
    "        tensor = self.rms_norm2(tensor_skip)\n",
    "        tensor_skip = tensor_skip + self.mlp(tensor)\n",
    "        return tensor_skip.transpose(0, 1)\n",
    "\n",
    "class MyLlama(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_layer: int, num_heads: int = None):\n",
    "        super().__init__()\n",
    "        if num_heads == None: # Default is to use a head for every 64 values\n",
    "            self.num_heads = int(embedding_dim/64)\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer = nn.ModuleList()\n",
    "        for i in range(num_layer):\n",
    "            self.transformer.append(MyLlamaLayer(embedding_dim, self.num_heads))\n",
    "\n",
    "        self.norm = RMSNorm(embedding_dim)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(embedding_dim, num_embeddings)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, padding_mask: torch.Tensor):\n",
    "        # Creating padding mask\n",
    "        batch_size, sequence_length = padding_mask.shape\n",
    "        padding_mask = padding_mask.unsqueeze(1)  # [batch_size, 1, sequence_length]\n",
    "        padding_mask = padding_mask.expand(batch_size, sequence_length, sequence_length)  # [batch_size, sequence_length, sequence_length]\n",
    "\n",
    "        # Create attention masking before doing anything\n",
    "        shape = (tensor.shape[0], tensor.shape[1], tensor.shape[1])\n",
    "        causal_mask = torch.ones(shape, dtype=torch.int64).to(device)\n",
    "        causal_mask = torch.tril(causal_mask)\n",
    "\n",
    "        # Apply padding mask\n",
    "        mask = causal_mask & padding_mask\n",
    "        mask = torch.where(mask == 0, float('-inf'), mask)\n",
    "        mask = mask.to(dtype=torch.float32)\n",
    "\n",
    "        # Reshape to apply attention masking to each head\n",
    "        batch_list = list(range(batch_size))\n",
    "        indices = torch.tensor(batch_list).repeat_interleave(self.num_heads)\n",
    "        mask = mask[indices]\n",
    "\n",
    "        # Transformer\n",
    "        for layer in self.transformer:\n",
    "            tensor = layer(tensor, mask)\n",
    "\n",
    "        tensor = self.norm(tensor)\n",
    "\n",
    "        # Classifier\n",
    "        return self.classifier(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62zMDqiMp2JY"
   },
   "outputs": [],
   "source": [
    "llama = MyLlama(embedding_dim, num_layer).to(device)\n",
    "print(\"This model has\", sum(p.numel() for p in llama.parameters()), \"parameters.\")\n",
    "scaler = amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAyAwdsUgemj"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYGAAPVV4EWi"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(llama.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTOtvAUfFm0Z"
   },
   "outputs": [],
   "source": [
    "loss_train = []\n",
    "loss_valid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiEXLV85gg6g"
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_train_epoch = []\n",
    "    loss_val_epoch = []\n",
    "    for batch_idx, data in enumerate(tqdm(training_loader)):\n",
    "        # Clear out grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Teacher forcing\n",
    "        input_data = data[0][:, :-1]\n",
    "        target_data = data[0][:, 1:].to(device)\n",
    "        padding_mask = data[1][:, :-1].to(device)\n",
    "\n",
    "        # Convert to embedding.\n",
    "        input_embeddings = word_embeddings_tensor[input_data]\n",
    "        input_embeddings = input_embeddings + pos_matrix\n",
    "        input_data = input_data.to(device)\n",
    "        input_embeddings = input_embeddings.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with amp.autocast():\n",
    "            prediction = llama(input_embeddings, padding_mask)\n",
    "\n",
    "            # Change shape for loss calculation\n",
    "            prediction = prediction.view(-1, num_embeddings)\n",
    "            target_data = target_data.reshape(-1)\n",
    "            loss = criterion(prediction, target_data) # Calculate loss\n",
    "            scaler.scale(loss/gradient_accumulation_step).backward()\n",
    "\n",
    "        # Backward pass\n",
    "        if (batch_idx + 1) % gradient_accumulation_step == 0 or (batch_idx + 1) == len(training_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        # Record loss\n",
    "        loss_train_epoch.append(loss.item())\n",
    "\n",
    "    loss_train.append(np.mean(loss_train_epoch))\n",
    "\n",
    "    for data in tqdm(validation_loader):\n",
    "        # Teacher forcing\n",
    "        input_data = data[0][:, :-1]\n",
    "        target_data = data[0][:, 1:].to(device)\n",
    "        padding_mask = data[1][:, :-1].to(device)\n",
    "\n",
    "        # Convert to embedding.\n",
    "        input_embeddings = word_embeddings_tensor[input_data]\n",
    "        input_embeddings = input_embeddings + pos_matrix\n",
    "        input_data = input_data.to(device)\n",
    "        input_embeddings = input_embeddings.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with amp.autocast():\n",
    "            prediction = llama(input_embeddings, padding_mask)\n",
    "\n",
    "            # Change shape for loss calculation\n",
    "            prediction = prediction.view(-1, num_embeddings)\n",
    "            target_data = target_data.reshape(-1)\n",
    "            loss = criterion(prediction, target_data) # Calculate loss\n",
    "\n",
    "        # Record loss\n",
    "        loss_val_epoch.append(loss.item())\n",
    "\n",
    "    loss_valid.append(np.mean(loss_val_epoch))\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if len(loss_train) >= 2:\n",
    "        plt.plot(loss_train[1:], label=\"Training loss\")\n",
    "        plt.plot(loss_valid[1:], label=\"Validation loss\")\n",
    "        print(\"Training loss: \", loss_train[-1])\n",
    "        print(\"Validation loss: \", loss_valid[-1])\n",
    "    else:\n",
    "        plt.plot(loss_train, label=\"Training loss\")\n",
    "        plt.plot(loss_valid, label=\"Validation loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = \"An apple is a round, edible fruit produced by\"\n",
    "tokenized_sentence = tokenizer(sentence)[\"input_ids\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    while(tokenized_sentence[-1] != tokenizer.eos_token_id and len(tokenized_sentence) < max_token): # Keep iterating until reaches end of sentence or max token limit\n",
    "        # Preparing input\n",
    "        tokenized_sentence_tensor = torch.tensor(tokenized_sentence)\n",
    "        sentence_embedding = word_embeddings_tensor[tokenized_sentence_tensor] + pos_matrix[:len(tokenized_sentence_tensor)].unsqueeze(0)\n",
    "        sentence_embedding = sentence_embedding.to(device)\n",
    "        attention_padding = torch.ones(len(tokenized_sentence_tensor), dtype=torch.int64).unsqueeze(0).to(device)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = llama(sentence_embedding, attention_padding)\n",
    "        prediction = prediction[0][-1] # We only care about last token\n",
    "        prediction = prediction / temperature\n",
    "        prediction = F.softmax(prediction, dim=-1)\n",
    "        output_token = torch.multinomial(prediction, 1)\n",
    "\n",
    "        # Append to conversation history\n",
    "        tokenized_sentence.append(output_token.item())\n",
    "\n",
    "tokens = tokenizer.decode(tokenized_sentence)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
