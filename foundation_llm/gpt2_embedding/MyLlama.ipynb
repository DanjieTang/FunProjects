{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OvBqu1zyKOqR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.venv/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./.venv/lib/python3.10/site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.venv/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in ./.venv/lib/python3.10/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in ./.venv/lib/python3.10/site-packages (from datasets) (0.20.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.venv/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.36.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.venv/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.venv/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.venv/lib/python3.10/site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.26.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.10/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./.venv/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: triton==2.1.0 in ./.venv/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.10/site-packages (8.1.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.10/site-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.10/site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in ./.venv/lib/python3.10/site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in ./.venv/lib/python3.10/site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: exceptiongroup in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.10/site-packages (0.20.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install nltk\n",
    "!pip install matplotlib\n",
    "!pip install torch\n",
    "!pip install ipywidgets\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "seN3tFvRFjeX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/danjie_tang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from huggingface_hub import notebook_login\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from torch.cuda import amp\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qTzjE8UgnuXV"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_token = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "epochs = 100\n",
    "batch_size = 50\n",
    "validation_batch_size = 100\n",
    "weight_decay = 1e-3\n",
    "drop_out_rate = 0.5\n",
    "lr = 1e-3\n",
    "gamma = 0.8\n",
    "num_layer = 24\n",
    "gradient_accumulation_step = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_hpclrXQVxG"
   },
   "source": [
    "# Load llama2 and use its tokenizer and word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DfO7Ax2pFbnv"
   },
   "outputs": [],
   "source": [
    "# Download gpt2 and its tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Store all word embedding\n",
    "word_embeddings = model.transformer.wte\n",
    "\n",
    "# Delete gpt2 becase we are no longer using it.\n",
    "del model\n",
    "\n",
    "# Store how many embedding and embedding dimension of our gpt2\n",
    "num_embeddings, embedding_dim = word_embeddings.weight.size()\n",
    "\n",
    "# Create a padding embedding and initialize the padding embedding with zeros\n",
    "padding_embedding = torch.nn.Embedding(1, embedding_dim)\n",
    "padding_embedding.weight.data.zero_()\n",
    "num_embeddings += 1\n",
    "\n",
    "# Concatenate the new padding embedding with the existing word embeddings\n",
    "word_embeddings_tensor = torch.cat([word_embeddings.weight, padding_embedding.weight], 0)\n",
    "word_embeddings_tensor = word_embeddings_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKae-ChZQTH6"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8n_DBC9am4aa"
   },
   "outputs": [],
   "source": [
    "tokenized_data = []\n",
    "attention_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ttQJS0ZaloO8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danjie_tang/Documents/GitHub/FunProjects/foundation_llm/.venv/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "  0%|                                               | 0/6458670 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(input_ids)):\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_ids[j]) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_token:\n\u001b[0;32m---> 19\u001b[0m             \u001b[43mtokenized_data\u001b[49m\u001b[38;5;241m.\u001b[39mappend(input_ids[j])\n\u001b[1;32m     20\u001b[0m             attention_data\u001b[38;5;241m.\u001b[39mappend(attention_mask[j])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Write into json\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\")\n",
    "training_dataset = dataset[\"train\"]\n",
    "\n",
    "# Tokenize all training data and filter those longer than token limit\n",
    "for i in tqdm(range(len(training_dataset))):\n",
    "    text = training_dataset[i][\"text\"]\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentences = [sentence+\"<|endoftext|>\" for sentence in sentences]\n",
    "\n",
    "    # Tokenize input\n",
    "    tokenized_sentence = tokenizer(sentences, padding='max_length', max_length=max_token)\n",
    "    input_ids = tokenized_sentence[\"input_ids\"]\n",
    "    attention_mask = tokenized_sentence[\"attention_mask\"]\n",
    "\n",
    "    # Filter those longer than max_token\n",
    "    for j in range(len(input_ids)):\n",
    "        if len(input_ids[j]) <= max_token:\n",
    "            tokenized_data.append(input_ids[j])\n",
    "            attention_data.append(attention_mask[j])\n",
    "\n",
    "# Write into json\n",
    "with open('tokenized_data.json', 'w') as file:\n",
    "    # Write the JSON data\n",
    "    json.dump(tokenized_data, file)\n",
    "\n",
    "with open('attention_data.json', 'w') as file:\n",
    "    # Write the JSON data\n",
    "    json.dump(attention_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from json (If you had already preprocessed\n",
    "with open('tokenized_data.json', 'r') as file:\n",
    "    # Load the data from the file\n",
    "    tokenized_data = json.load(file)\n",
    "\n",
    "with open('attention_data.json', 'r') as file:\n",
    "    # Load the data from the file\n",
    "    attention_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_Rk3HQA4wJVk"
   },
   "outputs": [],
   "source": [
    "total_data_num = len(tokenized_data)\n",
    "training_data_num = int(total_data_num * 0.95)\n",
    "\n",
    "training_data = torch.tensor(tokenized_data[:training_data_num])\n",
    "training_attention = torch.tensor(attention_data[:training_data_num])\n",
    "validation_data = torch.tensor(tokenized_data[training_data_num:])\n",
    "validation_attention = torch.tensor(attention_data[training_data_num:])\n",
    "\n",
    "# Create a TensorDataset\n",
    "training_data = TensorDataset(training_data, training_attention)\n",
    "validation_data = TensorDataset(validation_data, validation_attention)\n",
    "\n",
    "# Use DataLoader for batching, etc.\n",
    "training_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=validation_batch_size, shuffle=True)\n",
    "\n",
    "# Free up memory\n",
    "del tokenized_data\n",
    "del attention_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_OH9l-ln-Vt"
   },
   "source": [
    "# Create positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XsFa_6UDnpRE"
   },
   "outputs": [],
   "source": [
    "# Positional encoding\n",
    "max_token_pos = max_token - 1\n",
    "pos_matrix = torch.empty(max_token_pos, embedding_dim)\n",
    "for i in range(max_token_pos):\n",
    "    for j in range(0, embedding_dim, 2):\n",
    "        pos_matrix[i, j] = np.sin(i/(10000**(j/embedding_dim)))\n",
    "        if(j+1<embedding_dim):\n",
    "            pos_matrix[i, j+1] = np.cos(i/(10000**(j/embedding_dim)))\n",
    "pos_matrix = pos_matrix.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGccxpfWY3OX"
   },
   "source": [
    "# Instantiate LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "M9IrxjMxY82i"
   },
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return tensor * torch.sigmoid(tensor)\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.V = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.swish = Swish()\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        W = self.W(tensor)\n",
    "        V = self.V(tensor)\n",
    "        return self.swish(W) * V\n",
    "\n",
    "# This is root mean square norm implementation by author\n",
    "# I do not take any credit for this\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, p=-1., eps=1e-8, bias=False):\n",
    "        \"\"\"\n",
    "            Root Mean Square Layer Normalization\n",
    "        :param d: model size\n",
    "        :param p: partial RMSNorm, valid value [0, 1], default -1.0 (disabled)\n",
    "        :param eps:  epsilon value, default 1e-8\n",
    "        :param bias: whether use bias term for RMSNorm, disabled by\n",
    "            default because RMSNorm doesn't enforce re-centering invariance.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.bias = bias\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones(d))\n",
    "        self.register_parameter(\"scale\", self.scale)\n",
    "\n",
    "        if self.bias:\n",
    "            self.offset = nn.Parameter(torch.zeros(d))\n",
    "            self.register_parameter(\"offset\", self.offset)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.p < 0. or self.p > 1.:\n",
    "            norm_x = x.norm(2, dim=-1, keepdim=True)\n",
    "            d_x = self.d\n",
    "        else:\n",
    "            partial_size = int(self.d * self.p)\n",
    "            partial_x, _ = torch.split(x, [partial_size, self.d - partial_size], dim=-1)\n",
    "\n",
    "            norm_x = partial_x.norm(2, dim=-1, keepdim=True)\n",
    "            d_x = partial_size\n",
    "\n",
    "        rms_x = norm_x * d_x ** (-1. / 2)\n",
    "        x_normed = x / (rms_x + self.eps)\n",
    "\n",
    "        if self.bias:\n",
    "            return self.scale * x_normed + self.offset\n",
    "\n",
    "        return self.scale * x_normed\n",
    "\n",
    "class MyLlamaLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, expand_factor: int = 4):\n",
    "        super().__init__()\n",
    "        # Transformer layer\n",
    "        self.rms_norm1 = RMSNorm(embedding_dim)\n",
    "        self.multihead_attention = nn.MultiheadAttention(embedding_dim, num_heads=num_heads)\n",
    "        self.rms_norm2 = RMSNorm(embedding_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim*expand_factor),\n",
    "            RMSNorm(embedding_dim*expand_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_out_rate),\n",
    "            RMSNorm(embedding_dim*expand_factor),\n",
    "            nn.Linear(embedding_dim*expand_factor, embedding_dim),\n",
    "            nn.Dropout(drop_out_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        # Reshape to follow [seq_length, batch_size, embedding_size]\n",
    "        tensor = tensor.transpose(0, 1)\n",
    "\n",
    "        # Actually go through the transformer layer\n",
    "        tensor_skip = tensor\n",
    "        tensor = self.rms_norm1(tensor)\n",
    "        tensor_skip = tensor_skip + self.multihead_attention(tensor, tensor, tensor, attn_mask=mask)[0]\n",
    "        tensor = self.rms_norm2(tensor_skip)\n",
    "        tensor_skip = tensor_skip + self.mlp(tensor)\n",
    "        return tensor_skip.transpose(0, 1)\n",
    "\n",
    "class MyLlama(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_layer: int, num_heads: int = None):\n",
    "        super().__init__()\n",
    "        if num_heads == None: # Default is to use a head for every 64 values\n",
    "            self.num_heads = int(embedding_dim/64)\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer = nn.ModuleList()\n",
    "        for i in range(num_layer):\n",
    "            self.transformer.append(MyLlamaLayer(embedding_dim, self.num_heads))\n",
    "\n",
    "        self.norm = RMSNorm(embedding_dim)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(embedding_dim, num_embeddings)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, padding_mask: torch.Tensor):\n",
    "        # Creating padding mask\n",
    "        batch_size, sequence_length = padding_mask.shape\n",
    "        padding_mask = padding_mask.unsqueeze(1)  # [batch_size, 1, sequence_length]\n",
    "        padding_mask = padding_mask.expand(batch_size, sequence_length, sequence_length)  # [batch_size, sequence_length, sequence_length]\n",
    "\n",
    "        # Create attention masking before doing anything\n",
    "        shape = (tensor.shape[0], tensor.shape[1], tensor.shape[1])\n",
    "        causal_mask = torch.ones(shape, dtype=torch.int64).to(device)\n",
    "        causal_mask = torch.tril(causal_mask)\n",
    "\n",
    "        # Apply padding mask\n",
    "        mask = causal_mask & padding_mask\n",
    "        mask = torch.where(mask == 0, float('-inf'), mask)\n",
    "        mask = mask.to(dtype=torch.float32)\n",
    "\n",
    "        # Reshape to apply attention masking to each head\n",
    "        batch_list = list(range(batch_size))\n",
    "        indices = torch.tensor(batch_list).repeat_interleave(self.num_heads)\n",
    "        mask = mask[indices]\n",
    "\n",
    "        # Transformer\n",
    "        for layer in self.transformer:\n",
    "            tensor = layer(tensor, mask)\n",
    "\n",
    "        tensor = self.norm(tensor)\n",
    "\n",
    "        # Classifier\n",
    "        return self.classifier(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "62zMDqiMp2JY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 208868690 parameters.\n"
     ]
    }
   ],
   "source": [
    "llama = MyLlama(embedding_dim, num_layer).to(device)\n",
    "print(\"This model has\", sum(p.numel() for p in llama.parameters()), \"parameters.\")\n",
    "scaler = amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAyAwdsUgemj"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GYGAAPVV4EWi"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(llama.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bTOtvAUfFm0Z"
   },
   "outputs": [],
   "source": [
    "loss_train = []\n",
    "loss_valid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xiEXLV85gg6g"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                | 0/104866 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 1/104866 [00:00<25:56:19,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 2/104866 [00:01<17:01:03,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 3/104866 [00:01<14:08:31,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 4/104866 [00:01<12:47:13,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 5/104866 [00:02<12:02:51,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 6/104866 [00:02<11:34:23,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 7/104866 [00:03<11:17:14,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 8/104866 [00:03<12:23:06,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 9/104866 [00:03<11:53:33,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 10/104866 [00:04<11:30:54,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 11/104866 [00:04<11:16:33,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 12/104866 [00:05<11:08:41,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 13/104866 [00:05<11:06:07,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 14/104866 [00:05<11:03:45,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 15/104866 [00:06<10:59:23,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 16/104866 [00:06<11:45:15,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 17/104866 [00:07<11:29:16,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 18/104866 [00:07<11:14:56,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 19/104866 [00:07<11:10:40,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 20/104866 [00:08<11:06:53,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 21/104866 [00:08<11:04:59,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 22/104866 [00:08<11:00:51,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 23/104866 [00:09<10:57:28,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 24/104866 [00:09<11:43:47,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 25/104866 [00:10<11:29:30,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 31, 768])\n",
      "torch.Size([31, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 25/104866 [00:10<12:15:42,  2.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m     target_data \u001b[38;5;241m=\u001b[39m target_data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(prediction, target_data) \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mgradient_accumulation_step\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m gradient_accumulation_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(training_loader):\n",
      "File \u001b[0;32m~/Documents/GitHub/FunProjects/foundation_llm/.venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/FunProjects/foundation_llm/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_train_epoch = []\n",
    "    loss_val_epoch = []\n",
    "    for batch_idx, data in enumerate(tqdm(training_loader)):\n",
    "        # Clear out grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Teacher forcing\n",
    "        data[0] = data[0].to(device)\n",
    "        input_data = data[0][:, :-1]\n",
    "        target_data = data[0][:, 1:]\n",
    "        padding_mask = data[1][:, :-1].to(device)\n",
    "\n",
    "        # Convert to embedding.\n",
    "        input_embeddings = word_embeddings_tensor[input_data]\n",
    "        print(input_embeddings.shape)\n",
    "        print(pos_matrix.shape)\n",
    "        input_embeddings = input_embeddings + pos_matrix\n",
    "\n",
    "        # Forward pass\n",
    "        with amp.autocast():\n",
    "            prediction = llama(input_embeddings, padding_mask)\n",
    "\n",
    "            # Change shape for loss calculation\n",
    "            prediction = prediction.view(-1, num_embeddings)\n",
    "            target_data = target_data.reshape(-1)\n",
    "            loss = criterion(prediction, target_data) # Calculate loss\n",
    "            scaler.scale(loss/gradient_accumulation_step).backward()\n",
    "\n",
    "        # Backward pass\n",
    "        if (batch_idx + 1) % gradient_accumulation_step == 0 or (batch_idx + 1) == len(training_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        # Record loss\n",
    "        loss_train_epoch.append(loss.item())\n",
    "\n",
    "    loss_train.append(np.mean(loss_train_epoch))\n",
    "\n",
    "    for data in tqdm(validation_loader):\n",
    "        # Teacher forcing\n",
    "        data[0] = data[0].to(device)\n",
    "        input_data = data[0][:, :-1]\n",
    "        target_data = data[0][:, 1:]\n",
    "        padding_mask = data[1][:, :-1].to(device)\n",
    "\n",
    "        # Convert to embedding.\n",
    "        input_embeddings = word_embeddings_tensor[input_data]\n",
    "        input_embeddings = input_embeddings + pos_matrix\n",
    "\n",
    "        # Forward pass\n",
    "        with amp.autocast():\n",
    "            prediction = llama(input_embeddings, padding_mask)\n",
    "\n",
    "            # Change shape for loss calculation\n",
    "            prediction = prediction.view(-1, num_embeddings)\n",
    "            target_data = target_data.reshape(-1)\n",
    "            loss = criterion(prediction, target_data) # Calculate loss\n",
    "\n",
    "        # Record loss\n",
    "        loss_val_epoch.append(loss.item())\n",
    "\n",
    "    loss_valid.append(np.mean(loss_val_epoch))\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if len(loss_train) >= 2:\n",
    "        plt.plot(loss_train[1:], label=\"Training loss\")\n",
    "        plt.plot(loss_valid[1:], label=\"Validation loss\")\n",
    "        print(\"Training loss: \", loss_train[-1])\n",
    "        print(\"Validation loss: \", loss_valid[-1])\n",
    "    else:\n",
    "        plt.plot(loss_train, label=\"Training loss\")\n",
    "        plt.plot(loss_valid, label=\"Validation loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = \"An apple is a round, edible fruit produced by\"\n",
    "tokenized_sentence = tokenizer(sentence)[\"input_ids\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    while(tokenized_sentence[-1] != tokenizer.eos_token_id and len(tokenized_sentence) < max_token): # Keep iterating until reaches end of sentence or max token limit\n",
    "        # Preparing input\n",
    "        tokenized_sentence_tensor = torch.tensor(tokenized_sentence)\n",
    "        sentence_embedding = word_embeddings_tensor[tokenized_sentence_tensor] + pos_matrix[:len(tokenized_sentence_tensor)].unsqueeze(0)\n",
    "        attention_padding = torch.ones(len(tokenized_sentence_tensor), dtype=torch.int64).unsqueeze(0).to(device)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = llama(sentence_embedding, attention_padding)\n",
    "        prediction = prediction[0][-1] # We only care about last token\n",
    "        prediction = prediction / temperature\n",
    "        prediction = F.softmax(prediction, dim=-1)\n",
    "        output_token = torch.multinomial(prediction, 1)\n",
    "\n",
    "        # Append to conversation history\n",
    "        tokenized_sentence.append(output_token.item())\n",
    "\n",
    "tokens = tokenizer.decode(tokenized_sentence)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
