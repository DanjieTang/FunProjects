{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OvBqu1zyKOqR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.2)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Downloading pyarrow-14.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.65.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Collecting tzdata>=2022.1 (from pandas->datasets)\n",
      "  Downloading tzdata-2023.4-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.3/330.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-14.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tzdata, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, pandas, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.12.2\n",
      "    Uninstalling fsspec-2023.12.2:\n",
      "      Successfully uninstalled fsspec-2023.12.2\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.16.1 dill-0.3.7 frozenlist-1.4.1 fsspec-2023.10.0 huggingface-hub-0.20.2 multidict-6.0.4 multiprocess-0.70.15 pandas-2.1.4 pyarrow-14.0.2 pyarrow-hotfix-0.6 tzdata-2023.4 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2023.12.25 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.36.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: joblib, nltk\n",
      "Successfully installed joblib-1.3.2 nltk-3.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting matplotlib\n",
      "  Downloading matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.47.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (157 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.6/157.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.0.1)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.47.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.47.2 kiwisolver-1.4.5 matplotlib-3.8.2 pyparsing-3.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install nltk\n",
    "!pip install matplotlib\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "seN3tFvRFjeX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from huggingface_hub import notebook_login\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from torch.cuda import amp\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qTzjE8UgnuXV"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_token = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "epochs = 100\n",
    "batch_size = 150\n",
    "validation_batch_size = 100\n",
    "weight_decay = 1e-3\n",
    "drop_out_rate = 0.5\n",
    "lr = 1e-3\n",
    "gamma = 0.8\n",
    "num_layer = 24\n",
    "gradient_accumulation_step = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_hpclrXQVxG"
   },
   "source": [
    "# Load gpt2 and use its tokenizer and word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DfO7Ax2pFbnv"
   },
   "outputs": [],
   "source": [
    "# Download gpt2 and its tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Store all word embedding\n",
    "word_embeddings = model.transformer.wte\n",
    "\n",
    "# Delete gpt2 becase we are no longer using it.\n",
    "del model\n",
    "\n",
    "# Store how many embedding and embedding dimension of our gpt2\n",
    "num_embeddings, embedding_dim = word_embeddings.weight.size()\n",
    "\n",
    "# Create a padding embedding and initialize the padding embedding with zeros\n",
    "padding_embedding = torch.nn.Embedding(1, embedding_dim)\n",
    "padding_embedding.weight.data.zero_()\n",
    "num_embeddings += 1\n",
    "\n",
    "# Concatenate the new padding embedding with the existing word embeddings\n",
    "word_embeddings_tensor = torch.cat([word_embeddings.weight, padding_embedding.weight], 0)\n",
    "word_embeddings_tensor = word_embeddings_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKae-ChZQTH6"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8n_DBC9am4aa"
   },
   "outputs": [],
   "source": [
    "tokenized_data = []\n",
    "attention_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttQJS0ZaloO8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd06f33f72d43808ce95a7621b43a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/35.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e83bc00b79440c196e7b77a9bf7ce13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/30.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ac94964a494d84a19ed21ea9a0893d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a656781f024d4d60b20c5fbd06cf8acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/15.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8456397f927549e3a4666f9c6c6ac4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/20.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download dataset\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\")\n",
    "training_dataset = dataset[\"train\"]\n",
    "\n",
    "# Tokenize all training data and filter those longer than token limit\n",
    "for i in tqdm(range(len(training_dataset))):\n",
    "    text = training_dataset[i][\"text\"]\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentences = [sentence+\"<|endoftext|>\" for sentence in sentences]\n",
    "\n",
    "    # Tokenize input\n",
    "    tokenized_sentence = tokenizer(sentences, padding='max_length', max_length=max_token)\n",
    "    input_ids = tokenized_sentence[\"input_ids\"]\n",
    "    attention_mask = tokenized_sentence[\"attention_mask\"]\n",
    "\n",
    "    # Filter those longer than max_token\n",
    "    for j in range(len(input_ids)):\n",
    "        if len(input_ids[j]) <= max_token:\n",
    "            tokenized_data.append(input_ids[j])\n",
    "            attention_data.append(attention_mask[j])\n",
    "\n",
    "    if i == 100000:\n",
    "        break\n",
    "\n",
    "# Write into json\n",
    "with open('tokenized_data.json', 'w') as file:\n",
    "    # Write the JSON data\n",
    "    json.dump(tokenized_data, file)\n",
    "\n",
    "with open('attention_data.json', 'w') as file:\n",
    "    # Write the JSON data\n",
    "    json.dump(attention_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from json (If you had already preprocessed\n",
    "with open('tokenized_data.json', 'r') as file:\n",
    "    # Load the data from the file\n",
    "    tokenized_data = json.load(file)\n",
    "\n",
    "with open('attention_data.json', 'r') as file:\n",
    "    # Load the data from the file\n",
    "    attention_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_Rk3HQA4wJVk"
   },
   "outputs": [],
   "source": [
    "total_data_num = len(tokenized_data)\n",
    "training_data_num = int(total_data_num * 0.95)\n",
    "\n",
    "training_data = torch.tensor(tokenized_data[:training_data_num])\n",
    "training_attention = torch.tensor(attention_data[:training_data_num])\n",
    "validation_data = torch.tensor(tokenized_data[training_data_num:])\n",
    "validation_attention = torch.tensor(attention_data[training_data_num:])\n",
    "\n",
    "# Create a TensorDataset\n",
    "training_data = TensorDataset(training_data, training_attention)\n",
    "validation_data = TensorDataset(validation_data, validation_attention)\n",
    "\n",
    "# (Optional) Use DataLoader for batching, etc.\n",
    "training_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=validation_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_OH9l-ln-Vt"
   },
   "source": [
    "# Create positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XsFa_6UDnpRE"
   },
   "outputs": [],
   "source": [
    "# Positional encoding\n",
    "max_token_pos = max_token - 1\n",
    "pos_matrix = torch.empty(max_token_pos, embedding_dim)\n",
    "for i in range(max_token_pos):\n",
    "    for j in range(0, embedding_dim, 2):\n",
    "        pos_matrix[i, j] = np.sin(i/(10000**(j/embedding_dim)))\n",
    "        if(j+1<embedding_dim):\n",
    "            pos_matrix[i, j+1] = np.cos(i/(10000**(j/embedding_dim)))\n",
    "pos_matrix = pos_matrix.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGccxpfWY3OX"
   },
   "source": [
    "# Instantiate LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M9IrxjMxY82i"
   },
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return tensor * torch.sigmoid(tensor)\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.V = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.swish = Swish()\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        W = self.W(tensor)\n",
    "        V = self.V(tensor)\n",
    "        return self.swish(W) * V\n",
    "\n",
    "# This is root mean square norm implementation by author\n",
    "# I do not take any credit for this\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, p=-1., eps=1e-8, bias=False):\n",
    "        \"\"\"\n",
    "            Root Mean Square Layer Normalization\n",
    "        :param d: model size\n",
    "        :param p: partial RMSNorm, valid value [0, 1], default -1.0 (disabled)\n",
    "        :param eps:  epsilon value, default 1e-8\n",
    "        :param bias: whether use bias term for RMSNorm, disabled by\n",
    "            default because RMSNorm doesn't enforce re-centering invariance.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.bias = bias\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones(d))\n",
    "        self.register_parameter(\"scale\", self.scale)\n",
    "\n",
    "        if self.bias:\n",
    "            self.offset = nn.Parameter(torch.zeros(d))\n",
    "            self.register_parameter(\"offset\", self.offset)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.p < 0. or self.p > 1.:\n",
    "            norm_x = x.norm(2, dim=-1, keepdim=True)\n",
    "            d_x = self.d\n",
    "        else:\n",
    "            partial_size = int(self.d * self.p)\n",
    "            partial_x, _ = torch.split(x, [partial_size, self.d - partial_size], dim=-1)\n",
    "\n",
    "            norm_x = partial_x.norm(2, dim=-1, keepdim=True)\n",
    "            d_x = partial_size\n",
    "\n",
    "        rms_x = norm_x * d_x ** (-1. / 2)\n",
    "        x_normed = x / (rms_x + self.eps)\n",
    "\n",
    "        if self.bias:\n",
    "            return self.scale * x_normed + self.offset\n",
    "\n",
    "        return self.scale * x_normed\n",
    "\n",
    "class MyLlamaLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, expand_factor: int = 4):\n",
    "        super().__init__()\n",
    "        # Transformer layer\n",
    "        self.rms_norm1 = RMSNorm(embedding_dim)\n",
    "        self.multihead_attention = nn.MultiheadAttention(embedding_dim, num_heads=num_heads)\n",
    "        self.rms_norm2 = RMSNorm(embedding_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim*expand_factor),\n",
    "            RMSNorm(embedding_dim*expand_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_out_rate),\n",
    "            RMSNorm(embedding_dim*expand_factor),\n",
    "            nn.Linear(embedding_dim*expand_factor, embedding_dim),\n",
    "            nn.Dropout(drop_out_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        # Reshape to follow [seq_length, batch_size, embedding_size]\n",
    "        tensor = tensor.transpose(0, 1)\n",
    "\n",
    "        # Actually go through the transformer layer\n",
    "        tensor_skip = tensor\n",
    "        tensor = self.rms_norm1(tensor)\n",
    "        tensor_skip = tensor_skip + self.multihead_attention(tensor, tensor, tensor, attn_mask=mask)[0]\n",
    "        tensor = self.rms_norm2(tensor_skip)\n",
    "        tensor_skip = tensor_skip + self.mlp(tensor)\n",
    "        return tensor_skip.transpose(0, 1)\n",
    "\n",
    "class MyLlama(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_layer: int, num_heads: int = None):\n",
    "        super().__init__()\n",
    "        if num_heads == None: # Default is to use a head for every 64 values\n",
    "            self.num_heads = int(embedding_dim/64)\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer = nn.ModuleList()\n",
    "        for i in range(num_layer):\n",
    "            self.transformer.append(MyLlamaLayer(embedding_dim, self.num_heads))\n",
    "\n",
    "        self.norm = RMSNorm(embedding_dim)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(embedding_dim, num_embeddings)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, padding_mask: torch.Tensor):\n",
    "        # Creating padding mask\n",
    "        batch_size, sequence_length = padding_mask.shape\n",
    "        padding_mask = padding_mask.unsqueeze(1)  # [batch_size, 1, sequence_length]\n",
    "        padding_mask = padding_mask.expand(batch_size, sequence_length, sequence_length)  # [batch_size, sequence_length, sequence_length]\n",
    "\n",
    "        # Create attention masking before doing anything\n",
    "        shape = (tensor.shape[0], tensor.shape[1], tensor.shape[1])\n",
    "        causal_mask = torch.ones(shape, dtype=torch.int64).to(device)\n",
    "        causal_mask = torch.tril(causal_mask)\n",
    "\n",
    "        # Apply padding mask\n",
    "        mask = causal_mask & padding_mask\n",
    "        mask = torch.where(mask == 0, float('-inf'), mask)\n",
    "        mask = mask.to(dtype=torch.float32)\n",
    "\n",
    "        # Reshape to apply attention masking to each head\n",
    "        batch_list = list(range(batch_size))\n",
    "        indices = torch.tensor(batch_list).repeat_interleave(self.num_heads)\n",
    "        mask = mask[indices]\n",
    "\n",
    "        # Transformer\n",
    "        for layer in self.transformer:\n",
    "            tensor = layer(tensor, mask)\n",
    "\n",
    "        tensor = self.norm(tensor)\n",
    "\n",
    "        # Classifier\n",
    "        return self.classifier(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "62zMDqiMp2JY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 208868690 parameters.\n"
     ]
    }
   ],
   "source": [
    "llama = MyLlama(embedding_dim, num_layer).to(device)\n",
    "print(\"This model has\", sum(p.numel() for p in llama.parameters()), \"parameters.\")\n",
    "scaler = amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAyAwdsUgemj"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GYGAAPVV4EWi"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(llama.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bTOtvAUfFm0Z"
   },
   "outputs": [],
   "source": [
    "loss_train = []\n",
    "loss_valid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiEXLV85gg6g"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 330/34956 [01:40<2:57:10,  3.26it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    loss_train_epoch = []\n",
    "    loss_val_epoch = []\n",
    "    for batch_idx, data in enumerate(tqdm(training_loader)):\n",
    "        # Clear out grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Teacher forcing\n",
    "        data[0] = data[0].to(device)\n",
    "        input_data = data[0][:, :-1]\n",
    "        target_data = data[0][:, 1:]\n",
    "        padding_mask = data[1][:, :-1].to(device)\n",
    "\n",
    "        # Convert to embedding.\n",
    "        input_embeddings = word_embeddings_tensor[input_data]\n",
    "        input_embeddings = input_embeddings + pos_matrix\n",
    "\n",
    "        # Forward pass\n",
    "        with amp.autocast():\n",
    "            prediction = llama(input_embeddings, padding_mask)\n",
    "\n",
    "            # Change shape for loss calculation\n",
    "            prediction = prediction.view(-1, num_embeddings)\n",
    "            target_data = target_data.reshape(-1)\n",
    "            loss = criterion(prediction, target_data) # Calculate loss\n",
    "            scaler.scale(loss/gradient_accumulation_step).backward()\n",
    "\n",
    "        # Backward pass\n",
    "        if (batch_idx + 1) % gradient_accumulation_step == 0 or (batch_idx + 1) == len(training_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        # Record loss\n",
    "        loss_train_epoch.append(loss.item())\n",
    "\n",
    "    loss_train.append(np.mean(loss_train_epoch))\n",
    "\n",
    "    for data in tqdm(validation_loader):\n",
    "        # Teacher forcing\n",
    "        data[0] = data[0].to(device)\n",
    "        input_data = data[0][:, :-1]\n",
    "        target_data = data[0][:, 1:]\n",
    "        padding_mask = data[1][:, :-1].to(device)\n",
    "\n",
    "        # Convert to embedding.\n",
    "        input_embeddings = word_embeddings_tensor[input_data]\n",
    "        input_embeddings = input_embeddings + pos_matrix\n",
    "\n",
    "        # Forward pass\n",
    "        with amp.autocast():\n",
    "            prediction = llama(input_embeddings, padding_mask)\n",
    "\n",
    "            # Change shape for loss calculation\n",
    "            prediction = prediction.view(-1, num_embeddings)\n",
    "            target_data = target_data.reshape(-1)\n",
    "            loss = criterion(prediction, target_data) # Calculate loss\n",
    "\n",
    "        # Record loss\n",
    "        loss_val_epoch.append(loss.item())\n",
    "\n",
    "    loss_valid.append(np.mean(loss_val_epoch))\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if len(loss_train) >= 2:\n",
    "        plt.plot(loss_train[1:], label=\"Training loss\")\n",
    "        plt.plot(loss_valid[1:], label=\"Validation loss\")\n",
    "        print(\"Training loss: \", loss_train[-1])\n",
    "        print(\"Validation loss: \", loss_valid[-1])\n",
    "    else:\n",
    "        plt.plot(loss_train, label=\"Training loss\")\n",
    "        plt.plot(loss_valid, label=\"Validation loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An apple is a round, edible fruit produced by the state's economy and the city.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"An apple is a round, edible fruit produced by\"\n",
    "tokenized_sentence = tokenizer(sentence)[\"input_ids\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    while(tokenized_sentence[-1] != tokenizer.eos_token_id and len(tokenized_sentence) < max_token): # Keep iterating until reaches end of sentence or max token limit\n",
    "        # Preparing input\n",
    "        tokenized_sentence_tensor = torch.tensor(tokenized_sentence)\n",
    "        sentence_embedding = word_embeddings_tensor[tokenized_sentence_tensor] + pos_matrix[:len(tokenized_sentence_tensor)].unsqueeze(0)\n",
    "        attention_padding = torch.ones(len(tokenized_sentence_tensor), dtype=torch.int64).unsqueeze(0).to(device)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = llama(sentence_embedding, attention_padding)\n",
    "        prediction = prediction[0][-1] # We only care about last token\n",
    "        prediction = prediction / temperature\n",
    "        prediction = F.softmax(prediction, dim=-1)\n",
    "        output_token = torch.multinomial(prediction, 1)\n",
    "\n",
    "        # Append to conversation history\n",
    "        tokenized_sentence.append(output_token.item())\n",
    "\n",
    "tokens = tokenizer.decode(tokenized_sentence)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
