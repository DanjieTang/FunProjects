{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KTUgJpPkGcVk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: dill in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (2.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (0.19.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets>=2.0.0->evaluate) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (2.0.1+cu118)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.19.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.3.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (2023.5.0)\n",
      "Requirement already satisfied: requests in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.2.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danji\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install -U accelerate\n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IAycdWvJMVDW"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, concatenate_datasets, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import re\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sV12GFMZaiNT"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_list(dataset: Dataset) -> list[str]:\n",
    "    \"\"\"\n",
    "    Changes a dataset of conversation data into a list of single turn chat message.\n",
    "    \n",
    "    :param dataset: The dataset containing chit chat conversation message.\n",
    "    :return: A list of chat message.\n",
    "    \"\"\"\n",
    "    list_of_tweet = []\n",
    "    for row in dataset:\n",
    "        data = row[\"dialog\"]\n",
    "        list_of_tweet.extend(data)\n",
    "    return list_of_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kezDbvbA8jk4"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_trump = load_dataset(\"rguo123/trump_tweets\")\n",
    "dataset_twitter = load_dataset(\"daily_dialog\")\n",
    "\n",
    "# Create validation and testing dataset for trump tweets.\n",
    "dataset_size = len(dataset_trump['train'])\n",
    "train_size = int(0.8 * dataset_size)\n",
    "valid_size = int(0.1 * dataset_size)\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffled_dataset = dataset_trump['train'].shuffle(seed=0)\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset = shuffled_dataset.select(range(train_size))\n",
    "valid_dataset = shuffled_dataset.select(range(train_size, train_size + valid_size))\n",
    "test_dataset = shuffled_dataset.select(range(train_size + valid_size, dataset_size))\n",
    "\n",
    "# Create a new DatasetDict\n",
    "dataset_trump = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': valid_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Preprocessing for false Trump tweets\n",
    "train_data = {\"tweet\": dataset_to_list(dataset_twitter[\"train\"])}\n",
    "validation_data = {\"tweet\": dataset_to_list(dataset_twitter[\"validation\"])}\n",
    "test_data = {\"tweet\": dataset_to_list(dataset_twitter[\"test\"])}\n",
    "\n",
    "train_data = Dataset.from_dict(train_data)\n",
    "validation_data = Dataset.from_dict(validation_data)\n",
    "test_data = Dataset.from_dict(test_data)\n",
    "\n",
    "dataset_twitter = DatasetDict({\n",
    "        \"train\": train_data,\n",
    "        \"validation\": validation_data,\n",
    "        \"test\": test_data\n",
    "    } \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OphYN3oANdV0"
   },
   "outputs": [],
   "source": [
    "# Tokenize datasets functions\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer) # using dynamic padding\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"tweet\"], truncation=True)\n",
    "\n",
    "def true_trump_mapping(row):\n",
    "    return {\"labels\": 1}\n",
    "\n",
    "def false_trump_mapping(row):\n",
    "    return {\"labels\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Qv0cz7F_OX00"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c139a842e15241cbbf54ce1af2cc09df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bcc3c1fed634ec18dcdba517e2f5d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df8ab3018274286ae41b050422c0b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6ebff1d0154b598dd419dfb1d0b3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8069 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf97143e874424a95387e2347fbb3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7740 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46de0bef56f4312855aa59e2e5e1185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f581ff26df4a49278fe16f9036cb1f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8069 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a645256c564d7685ac6081c6625391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7740 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This if statement is to make sure it is only run once. \n",
    "if \"tweet\" not in dataset_trump[\"train\"].column_names:\n",
    "    # Rename the actual tweet column in each dataset as tweet for tokenization\n",
    "    dataset_trump = dataset_trump.rename_column('content', 'tweet')\n",
    "\n",
    "    # Remove every column other than tweet\n",
    "    trump_column_names = dataset_trump[\"train\"].column_names\n",
    "    trump_column_names.remove(\"tweet\")\n",
    "    dataset_trump = dataset_trump.remove_columns(trump_column_names)\n",
    "\n",
    "    # Prepare the dataset\n",
    "    tokenized_dataset_trump = dataset_trump.map(tokenize_function)\n",
    "    dataset_trump = tokenized_dataset_trump.map(true_trump_mapping)\n",
    "    tokenized_dataset_twitter = dataset_twitter.map(tokenize_function)\n",
    "    dataset_twitter = tokenized_dataset_twitter.map(false_trump_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Bsrsj8RVRJrf"
   },
   "outputs": [],
   "source": [
    "# Merge the two datasets.\n",
    "merge_dataset = DatasetDict({\n",
    "    'train': concatenate_datasets([dataset_trump[\"train\"], dataset_twitter[\"train\"]]),\n",
    "    'validation': concatenate_datasets([dataset_trump[\"validation\"], dataset_twitter[\"validation\"]]),\n",
    "    'test': concatenate_datasets([dataset_trump[\"test\"], dataset_twitter[\"test\"]]),\n",
    "})\n",
    "\n",
    "merge_dataset = merge_dataset.shuffle(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2101G-naaohL"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3JOo80dUapnB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cNW6nZQVbDVY"
   },
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "training_hyperparameter = TrainingArguments(output_dir=\"output_dir\",\n",
    "                                            evaluation_strategy=\"epoch\",\n",
    "                                            gradient_accumulation_steps=128,\n",
    "                                            per_device_train_batch_size=16,\n",
    "                                            per_device_eval_batch_size=16,\n",
    "                                            num_train_epochs=3,\n",
    "                                            warmup_steps=500,\n",
    "                                            learning_rate=2e-5,\n",
    "                                            weight_decay=1e-3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DyaPv2RWbGgK"
   },
   "outputs": [],
   "source": [
    "# Prepare for evaluation\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Pi6P1XUNb7gA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='177' max='177' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [177/177 50:22, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.397810</td>\n",
       "      <td>0.957191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.015610</td>\n",
       "      <td>0.994921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=177, training_loss=0.27076251358635683, metrics={'train_runtime': 3040.1635, 'train_samples_per_second': 120.241, 'train_steps_per_second': 0.058, 'total_flos': 1.191713836374336e+16, 'train_loss': 0.27076251358635683, 'epoch': 2.97})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_hyperparameter,\n",
    "    train_dataset=merge_dataset[\"train\"],\n",
    "    eval_dataset=merge_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer # using dynamic padding\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.9947830407419676\n"
     ]
    }
   ],
   "source": [
    "# Make prediction\n",
    "prediction = trainer.predict(merge_dataset[\"test\"])\n",
    "print(f\"Test accuracy is {prediction.metrics['test_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(tweet: str) -> None:\n",
    "    encoded_input = tokenizer(tweet, return_tensors='pt')\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    if torch.argmax(output.logits).item() == 0:\n",
    "        print(\"This is not a Trump tweet\")\n",
    "    else:\n",
    "        print(\"This is a Trump tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Trump tweet\n",
      "This is not a Trump tweet\n"
     ]
    }
   ],
   "source": [
    "# Examples\n",
    "trump_tweet = \"Make America great again!\"\n",
    "make_prediction(trump_tweet)\n",
    "my_tweet = \"Yo bro, what's up\"\n",
    "make_prediction(my_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it out yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Trump tweet\n"
     ]
    }
   ],
   "source": [
    "# Enter your sentence here\n",
    "sentence = \"Tonight, @FLOTUS and I tested positive for COVID-19. We will begin our quarantine and recovery process immediately. We will get through this TOGETHER!\"\n",
    "make_prediction(sentence)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
