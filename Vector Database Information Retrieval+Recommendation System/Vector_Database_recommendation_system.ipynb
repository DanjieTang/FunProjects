{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YrSvIiEW7w3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu==1.7.4 in ./.venv/lib/python3.11/site-packages (1.7.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (4.36.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.venv/lib/python3.11/site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.venv/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.venv/lib/python3.11/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.11/site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.11/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu==1.7.4  # FAISS can only load database from same FAISS version\n",
    "!pip install transformers\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Pk2XKbkU7zg8"
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "import openai\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vCSMmYmH7uC-"
   },
   "outputs": [],
   "source": [
    "class VectorDataBase:\n",
    "    \"\"\"\n",
    "    This class functions as a vector database\n",
    "\n",
    "    :param _storage: Stores the vector database\n",
    "    \"\"\"\n",
    "    _storage: faiss.Index\n",
    "    _id: np.ndarray\n",
    "    _metadata: np.ndarray\n",
    "    _review: np.ndarray\n",
    "    _metadata_storage: np.ndarray\n",
    "    _ntotal: int\n",
    "    _item: np.ndarray\n",
    "\n",
    "    def __init__(self, database_file_path: str, id_file_path: str, metadata_file_path: str, review_file_path: str, metadata_storage_file_path: str, item_file_path: np.ndarray):\n",
    "        self._storage = faiss.read_index(database_file_path)\n",
    "        self._id = np.load(id_file_path)\n",
    "        self._metadata = np.load(metadata_file_path)\n",
    "        self._review = np.load(review_file_path)\n",
    "        self._metadata_storage = np.load(metadata_storage_file_path, allow_pickle=True)\n",
    "        self._ntotal = self._storage.ntotal\n",
    "        self._item = np.load(item_file_path)\n",
    "\n",
    "    def search_for_index(self, query: np.ndarray, k: int):\n",
    "        \"\"\"\n",
    "        Search the database\n",
    "\n",
    "        :param query: This is the query vector\n",
    "        :param k: This is how many items to retrieve\n",
    "        :return: The indexs of most similar vectors\n",
    "        \"\"\"\n",
    "        # First output stores the distance between query and retrieved vectors\n",
    "        # I stores the index of retrieved vectors\n",
    "        _, I = self._storage.search(query, k)\n",
    "\n",
    "        return I\n",
    "\n",
    "    def search_for_vector(self, query: np.ndarray, k: int):\n",
    "        \"\"\"\n",
    "        Search the database and return the actual embedding vectors\n",
    "\n",
    "        :param query: This is the query vector\n",
    "        :param k: This is how many items to retrieve\n",
    "        :return: A numpy array containing the actual most similar vectors\n",
    "        \"\"\"\n",
    "        # First output stores the distance between query and retrieved vectors\n",
    "        # I stores the index of retrieved vectors\n",
    "        _, I = self._storage.search(query, k)\n",
    "\n",
    "        list_of_vectors = []\n",
    "        # Create the np array that contains the most similar vectors\n",
    "        for index in I[0]:\n",
    "            list_of_vectors.append(self._storage.reconstruct(int(index)))\n",
    "\n",
    "        np_of_vectors = np.array(list_of_vectors)\n",
    "\n",
    "        return np_of_vectors\n",
    "\n",
    "    def filter_with_id(self, target_id: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        This function serves as the filter for id\n",
    "\n",
    "        :param id: A 1d np array\n",
    "        :param target_id: A string representing the id you are searching for\n",
    "\n",
    "        :return: A numpy array with the same shape as id, with index of target_id\n",
    "        set to True while all other index set to false\n",
    "        \"\"\"\n",
    "        return self._id == target_id\n",
    "\n",
    "    def filter_with_metadata(self, target: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        This function examines the metadata and searches for lists that\n",
    "        include the target. It then generates a one-dimensional numpy array\n",
    "        where the index corresponds to each list. If a list contains the target,\n",
    "        the corresponding index in the array is set to True; otherwise,\n",
    "        it is set to False.\n",
    "\n",
    "        :param metadata: A 2d list containing metadata\n",
    "        :param target: A string representing the target we are filtering for\n",
    "\n",
    "        :return: A 1d numpy array with True represents this item's metadata\n",
    "        contains the target and False represent otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        indexes = np.zeros((self._ntotal), dtype=bool)\n",
    "\n",
    "        items_satisfies_requirement = []\n",
    "\n",
    "        for i, info in enumerate(self._metadata_storage):\n",
    "            for key in info.keys():\n",
    "                if isinstance(info[key], dict):\n",
    "                    if(target in info[key].values()):\n",
    "                        items_satisfies_requirement.append(i)\n",
    "                        break\n",
    "                else:\n",
    "                    if(target == info[key]):\n",
    "                        items_satisfies_requirement.append(i)\n",
    "                        break\n",
    "                    if isinstance(info[key], str):\n",
    "                        if(target in info[key]):\n",
    "                            items_satisfies_requirement.append(i)\n",
    "                            break\n",
    "\n",
    "        for number in items_satisfies_requirement:\n",
    "            id_filter = self._metadata == number\n",
    "            indexes = np.logical_or(indexes, id_filter)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def search_with_filter(self, query: np.ndarray, top_k_items: int, top_k_revirw:int,  target_id: list = None, target_metadata: list = None) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        This function filters the datavase to look for indexs with metadata\n",
    "        that contains the target we are looking for and items with id we are looking for.\n",
    "\n",
    "        :param query: The query embedding of shape [1, 768]\n",
    "        :param top_k_items: The number of items we want to return\n",
    "        :param top_k_revirw: The number of reviews we want to use for late fusion\n",
    "        :param target_id: The target id we are looking for\n",
    "        :param target_metadata: The metadata we are looking for\n",
    "\n",
    "        :return: The indexs of the review\n",
    "        \"\"\"\n",
    "        # Create id filter\n",
    "        id_filter = np.ones((self._ntotal), dtype=bool)\n",
    "        if(target_id != None):\n",
    "            # If user did not specify what id they are looking for\n",
    "            # we are not going to filter out anything\n",
    "            id_filter = np.zeros((self._ntotal), dtype=bool)\n",
    "            for id in target_id:\n",
    "                id_filter_requirement = self.filter_with_id(id)\n",
    "                id_filter = np.logical_or(id_filter, id_filter_requirement)\n",
    "\n",
    "        # Create metadata filter\n",
    "        metadata_filer = np.ones((self._ntotal), dtype=bool)\n",
    "        if(target_metadata != None):\n",
    "            # If user did not specify the kind of metadata they are looking for\n",
    "            # we are not going to filter out anything\n",
    "            for requirement in target_metadata:\n",
    "                metadata_filer_requirement = self.filter_with_metadata(requirement)\n",
    "                metadata_filer = np.logical_and(metadata_filer, metadata_filer_requirement)\n",
    "\n",
    "        mask = np.logical_and(id_filter, metadata_filer)\n",
    "\n",
    "        count = np.count_nonzero(mask == True)\n",
    "        if(count == 0):\n",
    "            # If the user specifies a filter that no item can satisfy\n",
    "            print(\"\"\"The filter you have entered appears to exclude all available\n",
    "            options. Please review your filter criteria to ensure that it allows\n",
    "            for the selection of relevant items.\"\"\")\n",
    "            return None\n",
    "        if(count < top_k_items):\n",
    "            # If the user ask for more retrieved item than there is\n",
    "            print(\"\"\"The number of items you want to retrieve is more than number of items that satisfies\n",
    "            your requirements.\"\"\")\n",
    "            return None\n",
    "\n",
    "        # Actually searching\n",
    "        top_k_item_index, top_k_item_most_similar_review = self.most_similar_item(query, top_k_items, top_k_revirw, mask)\n",
    "        return top_k_item_index, top_k_item_most_similar_review\n",
    "\n",
    "    def _find_similarity_vector(self, query: np.ndarray, filter: np.ndarray) -> np.ndarray:\n",
    "        query = query.reshape(1, self._storage.d)\n",
    "        D, I = self._storage.search(query, self._storage.ntotal)\n",
    "        D = D[0]\n",
    "        I = I[0] #For some reason FAISS return a numpy within a numpy that contains all the answer.\n",
    "\n",
    "        output = [False] * self._ntotal\n",
    "        for i, index in enumerate(I):\n",
    "            output[index] = D[i]\n",
    "\n",
    "        output = np.array(output)\n",
    "        output = output * filter\n",
    "        return output\n",
    "\n",
    "    def _find_similarity_item(self, similarity_score: np.ndarray, top_k_review: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        This function finds and returns a tensor that contains the similarity score for each item\n",
    "\n",
    "        :param similarity_score: A tensor of similarity score between each review and the query\n",
    "        :param top_k_review: A number that tells the number of most similar tensors to look at when doing late fusion(k)\n",
    "        :return: Returning a tuple with element 0 being a tensor that contains the similarity score for each item\n",
    "                and element 1 being a tensor that contains the index of top k reviews for each item\n",
    "        \"\"\"\n",
    "        similarity_score = torch.tensor(similarity_score)\n",
    "\n",
    "        index = 0\n",
    "        # size records how many items are in the matrix\n",
    "        size = self._item.shape[0]\n",
    "\n",
    "        item_score = []\n",
    "        item_index = []\n",
    "\n",
    "        for i in range(size):\n",
    "            # Mask out the review scores related to one item\n",
    "            similarity_score_item = similarity_score[index:index + self._item[i]]\n",
    "\n",
    "            # Get the top k review scores or all review scores if the number of reviews is less than k\n",
    "            values, index_topk = similarity_score_item.topk(top_k_review)\n",
    "\n",
    "            index_topk += index\n",
    "\n",
    "            # Get the item score by finding the mean of all the review scores\n",
    "            item_score.append(values.mean(dim=0))\n",
    "            item_index.append(index_topk.tolist())\n",
    "\n",
    "            index += self._item[i]\n",
    "\n",
    "        item_score = np.array(item_score)\n",
    "        item_index = np.array(item_index)\n",
    "        return item_score, item_index\n",
    "\n",
    "    def most_similar_item(self, query: np.ndarray, top_k_items: int, top_k_revirw: int, filter: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        This function returns the most similar item's index given the item similarity score\n",
    "\n",
    "        :param top_k_items: Number of items to return\n",
    "        :param top_k_revirw: Number of reviews to look at\n",
    "        :return: The indices of the most similar item, beginning from the most similar to the least similar,\n",
    "         and their corresponding most similar review index\n",
    "        \"\"\"\n",
    "        similarity_score_review = self._find_similarity_vector(query, filter)\n",
    "        similarity_score_item, most_similar_review_index = self._find_similarity_item(similarity_score_review, top_k_revirw)\n",
    "\n",
    "        # Unfortunately, numpy doesn't have finding the biggest elements in an array and return the indexs in\n",
    "        similarity_score_item = torch.tensor(similarity_score_item)\n",
    "        _, top_k_item_index = similarity_score_item.topk(top_k_items)\n",
    "\n",
    "        top_k_item_index = np.array(top_k_item_index)\n",
    "        top_k_item_most_similar_review = most_similar_review_index[top_k_item_index]\n",
    "\n",
    "        return top_k_item_index, top_k_item_most_similar_review\n",
    "\n",
    "    def get_database_size(self):\n",
    "        \"\"\"\n",
    "        This function finds how many vectors this database is storing\n",
    "\n",
    "        :return: The size of the database\n",
    "        \"\"\"\n",
    "        return self._ntotal\n",
    "\n",
    "    def get_vector_size(self):\n",
    "        \"\"\"\n",
    "        This function finds the size of the vector this database is storing\n",
    "\n",
    "        :return: The size of the vector\n",
    "        \"\"\"\n",
    "        return self._storage.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tOh6DEzPQPCa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open data.zip, data.zip.zip or data.zip.ZIP.\r\n"
     ]
    }
   ],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PRZ7t49aQNe8"
   },
   "outputs": [],
   "source": [
    "database = VectorDataBase(\"data/vector_database.faiss\", \"data/id.npy\", \"data/metadata.npy\", \"data/review.npy\", \"data/metadata_database.npy\", \"data/item.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "41Vh2LsOYYr2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# This is not my code!!!!!!!!\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import torch\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\"\"\"\n",
    "   Modified based on  https://github.com/D3Mlab/rir/blob/main/prefernce_matching/LM.py\n",
    "\"\"\"\n",
    "\n",
    "class BERT_model:\n",
    "\n",
    "    _BERT_name: str\n",
    "    _name1: str\n",
    "    _name2: str\n",
    "    _device: torch.device\n",
    "\n",
    "    def __init__(self, BERT_name, tokenizer_name, from_pt=False):\n",
    "        \"\"\"\n",
    "        :param BERT_name: name or address of language prefernce_matching\n",
    "        :param tokenizer_name: name or address of the tokenizer\n",
    "        \"\"\"\n",
    "        self._BERT_name = BERT_name\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self._bert_model, self._name1, self._name2 = self._create_model(BERT_name, from_pt)\n",
    "        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def embed(self, texts: list[str], strategy=None, bs=48, verbose=0):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        :param texts: list of strings to be embedded\n",
    "        :param strategy (optional): Defaults to None.\n",
    "        :param bs (optional): Defaults to 48.\n",
    "        :param verbose (optional): Defaults to 0.\n",
    "        :return: embeddings of texts\n",
    "        \"\"\"\n",
    "        tokenized_review = self._tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            max_length=512,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            # truncation_strategy='longest_first',\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "        )\n",
    "\n",
    "        data = {self._name1: tokenized_review['input_ids'],\n",
    "                self._name2: tokenized_review['attention_mask'],\n",
    "                # 'input_3': tokenized_review['token_type_ids']\n",
    "                }\n",
    "\n",
    "        if strategy is not None:\n",
    "            with strategy.scope():\n",
    "                dataset = tf.data.Dataset.from_tensor_slices(data).batch(bs, drop_remainder=False).prefetch(\n",
    "                    buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "                outputs = self._bert_model.predict(dataset, verbose=verbose)\n",
    "                return outputs['last_hidden_state'][:, 0, :].reshape(-1, 768)\n",
    "        else:\n",
    "            dataset = tf.data.Dataset.from_tensor_slices(data).prefetch(\n",
    "                buffer_size=tf.data.experimental.AUTOTUNE).batch(bs, drop_remainder=False)\n",
    "            outputs = self._bert_model.predict(dataset, verbose=verbose)\n",
    "            return outputs['last_hidden_state'][:, 0, :].reshape(-1, 768)\n",
    "\n",
    "    def get_tensor_embedding(self, query: str):\n",
    "        \"\"\"\n",
    "        Get a tensor embedding of a string.\n",
    "\n",
    "        :param query: string to be embedded\n",
    "        :return: tensor embedding of query\n",
    "        \"\"\"\n",
    "        query_embedding = self.embed([query])\n",
    "        query_embedding = torch.tensor(query_embedding).to(self._device)\n",
    "        query_embedding = query_embedding.squeeze(0)\n",
    "\n",
    "        return query_embedding\n",
    "\n",
    "    def _create_model(self, BERT_name, from_pt=True):\n",
    "        ## BERT encoder\n",
    "        encoder = TFAutoModel.from_pretrained(BERT_name, from_pt=True)\n",
    "\n",
    "        ## Model\n",
    "        input_ids = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "        attention_mask = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "        # token_type_ids = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "\n",
    "        embedding = encoder(\n",
    "            # input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        model = keras.Model(\n",
    "            # inputs=[input_ids, attention_mask, token_type_ids],\n",
    "            inputs = [input_ids, attention_mask],\n",
    "            outputs = embedding)\n",
    "\n",
    "        model.compile()\n",
    "        return model, input_ids.name, attention_mask.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LotQBUNVYkaB"
   },
   "outputs": [],
   "source": [
    "data_preprocessing=BERT_model(\"sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\", \"sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3NrKsPqi_RaA"
   },
   "outputs": [],
   "source": [
    "def create_response_single_item(item_index: int, review_index: list, query: str, requirements: list) -> str:\n",
    "    setting = \"You are a helpful assistant, helping me give a recommendation to a customer about this restaurant\"\n",
    "    concatenated_reviews = \"\"\n",
    "    for review in review_index:\n",
    "        concatenated_reviews += database._review[review]\n",
    "    prompt = \"These are the restaurant information: \" + str(database._metadata_storage[item_index]) + \". User sepeifies these requirements: \" + str(requirements) + \". This is user's query: \" + str(query) +\". These are the available reviews you can use: \"+ str(concatenated_reviews) + \"Help me generate a response explaining why this restaurant is a good choice.\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",  # Please verify the current version of GPT model on OpenAI's website\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": setting},\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "13VV7aF8A8mD"
   },
   "outputs": [],
   "source": [
    "def create_response_multiple_item(item_index: list, item_review: list, query: str, requirements: list):\n",
    "    concatenated_response = \"\"\n",
    "    for i in range(len(item_index)):\n",
    "        concatenated_response += create_response_single_item(item_index[i], item_review[i], query, requirements)\n",
    "\n",
    "    setting = \"You are a helpful assistant giving recommendation to a customer, these are the summaries of \" + str(len(item_index)) + \" restaurants. Help me generate a recommendation to customer explaining why all of these restaurants are a good choice, in a casual short conversation.\"\n",
    "    info = \"The names of the restaurants are as follows in respective order. \"\n",
    "    minor_setting = \"Give the recommendation in the next response. Do not mention I gave you the information above. Do not greet the customer, go straight to recommendation. Do not say things like 'Hey There!'\"\n",
    "    for i in range(len(item_index)):\n",
    "        info += database._metadata_storage[item_index[i]][\"name\"] + \", \"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Please verify the current version of GPT model on OpenAI's website\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": setting},\n",
    "            {\"role\": \"system\", \"content\": info},\n",
    "            {\"role\": \"user\", \"content\": concatenated_response},\n",
    "            {\"role\": \"system\", \"content\": minor_setting}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQcTheSKdlx4"
   },
   "source": [
    "# Welcome to the filtering demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38y8XFzge4rJ"
   },
   "source": [
    "Query: Enter restaurant description\n",
    "\n",
    "Requirements: Enter requirements that the restaurant MUST meet\n",
    "\n",
    "OpenAI api: Used to generate response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLDUIl6xYpHq",
    "outputId": "f1cf5476-d7f2-4298-864b-f0af29e42664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the reviews and information available, I\n",
      "have two great restaurants to recommend for you.\n",
      "The first one is \"I Love Sushi\" which offers\n",
      "authentic Japanese cuisine with a cozy atmosphere,\n",
      "friendly staff, and delicious, fresh food. It's a\n",
      "perfect spot for a quick lunch or a relaxed dinner\n",
      "with friends. They also have great takeout service\n",
      "if you want to enjoy their tasty dishes at home.\n",
      "The second recommendation is \"Zen All-You-Can-Eat\n",
      "Sushi & Grill\" which is known for its extensive\n",
      "all-you-can-eat sushi menu. Customers have praised\n",
      "the generous portion sizes, delicious food, and\n",
      "friendly service. They also offer non-sushi\n",
      "options to cater to different tastes. It might not\n",
      "have the fanciest ambiance, but it offers\n",
      "exceptional value for money with their affordable\n",
      "all-you-can-eat option. Plus, they even have fruit\n",
      "sushi, which is a unique and tasty twist.  Both of\n",
      "these restaurants provide a great dining\n",
      "experience, so you can choose the one that suits\n",
      "your preferences. Enjoy your meal!\n"
     ]
    }
   ],
   "source": [
    "# Enter restaurant description\n",
    "query = \"I want to have some sushi\" #@param {type:\"string\"}\n",
    "query = data_preprocessing.embed([query])\n",
    "\n",
    "# Enter requirements that you MUST have\n",
    "requirements = \"['Japanese']\" #@param {type:\"string\"}\n",
    "if(requirements == ''):\n",
    "    requirements = None\n",
    "else:\n",
    "    requirements = eval(requirements)\n",
    "\n",
    "# Enter your openai API key\n",
    "openai_api_key = \"''\" #@param {type:\"string\"}\n",
    "if(openai_api_key == \"\"):\n",
    "    raise Exception(\"No api key has been entered\")\n",
    "else:\n",
    "    openai_api_key = eval(openai_api_key)\n",
    "\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Number of restaurants to return\n",
    "restaurant_count = 2\n",
    "# Number of reviews to look at\n",
    "review_count = 2\n",
    "\n",
    "top_k_item_index, top_k_item_most_similar_review = database.search_with_filter(query, restaurant_count, review_count, target_metadata = requirements)\n",
    "\n",
    "recommendation = create_response_multiple_item(top_k_item_index, top_k_item_most_similar_review, query, requirements)\n",
    "print(textwrap.fill(recommendation, width=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmGuprm-NcUQ"
   },
   "outputs": [],
   "source": [
    "# Example outputs\n",
    "\n",
    "# Input query: I am so hungry, I want some food\n",
    "# Requirements: Chinese\n",
    "\n",
    "# I would recommend Jumbo Dim Sum\n",
    "# Dining for a Chinese dining experience. One\n",
    "# reviewer described the food as really good and the\n",
    "# portion size as generous, offering value for\n",
    "# money. While there was a comment about rudeness\n",
    "# from the staff, it is important to consider that\n",
    "# dim sum places often have fast-paced and efficient\n",
    "# service, which can sometimes come across as curt.\n",
    "# Overall, Jumbo Dim Sum Dining seems like a good\n",
    "# option for those seeking a satisfying, authentic\n",
    "# Chinese dining experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvL3cW3UOHZS"
   },
   "outputs": [],
   "source": [
    "# Example outputs\n",
    "\n",
    "# Input query: I want some yummy pizza\n",
    "# Requirements: Pizza\n",
    "\n",
    "# Both LovePizza and Famoso Neapolitan Pizzeria are\n",
    "# excellent choices for pizza lovers. LovePizza\n",
    "# stands out for its unique and delicious toppings,\n",
    "# such as truffle Parmesan sauce and Mac and cheese\n",
    "# pizza. The flavorful and substantial dough is also\n",
    "# a highlight. The staff is friendly and the modern\n",
    "# interior creates a hip atmosphere. On the other\n",
    "# hand, Famoso Neapolitan Pizzeria offers delicious\n",
    "# thin crust pizzas with rave reviews about the\n",
    "# Margarita Pizza and Ham and Pineapple Pizza with\n",
    "# fresh feta toppings. The artisan bread and\n",
    "# prosciutto-wrapped mozzarella balls are also\n",
    "# highly recommended. The atmosphere is great, and\n",
    "# the menu includes other tasty options like salads\n",
    "# and gelato. Overall, both restaurants guarantee a\n",
    "# satisfying and enjoyable pizza dining experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsEGLxN_Q4R_"
   },
   "outputs": [],
   "source": [
    "# Example outputs\n",
    "\n",
    "# Input query: I want to have breakfast\n",
    "# Requirements: Vegan\n",
    "\n",
    "# I've got a couple of great restaurant\n",
    "# recommendations for you based on your preferences.\n",
    "# The first one is Highlevel Diner. They offer a\n",
    "# wide variety of comfort food, sandwiches, and\n",
    "# Canadian cuisine, with a special focus on vegan\n",
    "# and vegetarian options. Their breakfast specials\n",
    "# are especially popular, with rave reviews about\n",
    "# the taste and quality. Plus, if you're into\n",
    "# cycling, they even offer a discount for customers\n",
    "# who arrive by bike.   The second option is Cafe\n",
    "# Mosaics, which is known for its delicious vegan\n",
    "# and vegetarian dishes. They have an extensive menu\n",
    "# featuring pancakes, tofu scramble, sandwiches,\n",
    "# burgers, and even vegan desserts like chocolate\n",
    "# cake. Customers have praised their fast service,\n",
    "# generous portions, and reasonable prices. The\n",
    "# atmosphere at Cafe Mosaics is cozy and the staff\n",
    "# is friendly, making it a great spot to enjoy a\n",
    "# satisfying meal.  Both Highlevel Diner and Cafe\n",
    "# Mosaics are highly recommended options that cater\n",
    "# to your preference for vegan-friendly restaurants.\n",
    "# So, whether you're craving a comforting Canadian\n",
    "# dish or a delicious vegan breakfast, you can't go\n",
    "# wrong with either of these choices. Enjoy your\n",
    "# meal!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
