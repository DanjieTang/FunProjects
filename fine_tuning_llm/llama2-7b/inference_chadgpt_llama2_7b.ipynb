{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chadgpt, llama2 7b version"
      ],
      "metadata": {
        "id": "nA7R7BiX9Wkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update  12/16/2023\n",
        "1. Execute the code cell below.\n",
        "2. After running the cell, restart your session.\n",
        "- **Reason:** Currently, a session restart is required for the model download. This might be a bug and could be patched in future releases.\n"
      ],
      "metadata": {
        "id": "lutAE_Lf73TX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yy9DVkQu0HTl"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install transformers\n",
        "!pip install -U accelerate\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes # Instal bits and bytes for inference of the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You need a huggingface token that can access llama2\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "2OLNvgqU08R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the model"
      ],
      "metadata": {
        "id": "WP_llh6x1Dbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = \"danjie/Chadgpt-Llama2-7b\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "id": "FcQ4DCNj0-8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "zTrNeUa81Aw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def talk_with_llm(tweet: str) -> str:\n",
        "    # Encode and move tensor into cuda if applicable.\n",
        "    encoded_input = tokenizer(tweet, return_tensors='pt')\n",
        "    encoded_input = {k: v.to(\"cuda\") for k, v in encoded_input.items()}\n",
        "\n",
        "    output = model.generate(**encoded_input, max_new_tokens=64)\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response"
      ],
      "metadata": {
        "id": "W6XTgG9b1AO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "talk_with_llm(\"<User> Hey, can you help me pick up my kids today? I'll have to run to a dentist appointment. \\n<Assistant>\")"
      ],
      "metadata": {
        "id": "WMeR0dao1E-S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}