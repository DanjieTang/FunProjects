{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCiLY8LulzfM"
   },
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes\n",
    "!pip install -U transformers\n",
    "!pip install -U accelerate\n",
    "!pip install -U loralib\n",
    "!pip3 install torch torchvision torchaudio\n",
    "!pip install -U datasets\n",
    "!pip install -U peft # This is to import PEFT\n",
    "!pip install -U scipy\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dsoiXUUml0I"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1u196njLoRRS"
   },
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_token = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need a huggingface token that can access llama2\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjYEDN3VoEew"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpcPtMD-oGQm"
   },
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # LLAMA2 does not have default padding token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdrqNdE7pVNe"
   },
   "outputs": [],
   "source": [
    "# Tokenization functions\n",
    "def tokenize_function(row):\n",
    "    return tokenizer(row[\"dialog\"], max_length=max_token, truncation=False)\n",
    "\n",
    "def is_shorter_than_max_token(row):\n",
    "    \"\"\"\n",
    "    Return if a given row has more than max_token number of tokens\n",
    "    \"\"\"\n",
    "    return len(row['input_ids']) <= max_token\n",
    "\n",
    "def split_conversation(conversation): \n",
    "    \"\"\"\n",
    "    Split conversation into turns\n",
    "    \"\"\"\n",
    "    return [conversation[:i+2] for i in range(0, len(conversation), 2) if i+2 <= len(conversation)]\n",
    "\n",
    "def format_conversation(conversation: list[str]) -> str:\n",
    "    formatted_conversation = \"\"\n",
    "    \n",
    "    # Check if the conversation has more than two turns\n",
    "    if len(conversation) > 2:\n",
    "        # Process all but the last two turns\n",
    "        for i in range(len(conversation) - 2):\n",
    "            if i % 2 == 0:\n",
    "                formatted_conversation += \"<Past User>\" + conversation[i] + \"\\n\"\n",
    "            else:\n",
    "                formatted_conversation += \"<Past Assistant>\" + conversation[i] + \"\\n\"\n",
    "    \n",
    "    # Process the last two turns\n",
    "    if len(conversation) >= 2:\n",
    "        formatted_conversation += \"<User>\" + conversation[-2] + \"\\n\"\n",
    "        formatted_conversation += \"<Assistant>\" + conversation[-1]\n",
    "    \n",
    "    return formatted_conversation\n",
    "\n",
    "def convert_to_conversation(row):\n",
    "    conversation_list = row[\"dialog\"]\n",
    "    \n",
    "    conversation = format_conversation(conversation_list)\n",
    "    conversation += \"</s>\"\n",
    "    return {\"dialog\": conversation.strip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxxO03Zfpi4w"
   },
   "outputs": [],
   "source": [
    "# Load and tokenize dataset\n",
    "dataset = load_dataset(\"daily_dialog\")\n",
    "\n",
    "# Split into multiple turns of conversation\n",
    "split_dataset = dataset.map(lambda x: {'dialog': split_conversation(x['dialog'])})\n",
    "\n",
    "# Flatten dataset\n",
    "flatten_dataset_train = [item for row in split_dataset[\"train\"][\"dialog\"] for item in row]\n",
    "flatten_dataset_valid = [item for row in split_dataset[\"validation\"][\"dialog\"] for item in row]\n",
    "flatten_dataset_test = [item for row in split_dataset[\"test\"][\"dialog\"] for item in row]\n",
    "\n",
    "flatten_dataset_train = Dataset.from_dict({'dialog': flatten_dataset_train})\n",
    "flatten_dataset_valid = Dataset.from_dict({'dialog': flatten_dataset_valid})\n",
    "flatten_dataset_test = Dataset.from_dict({'dialog': flatten_dataset_test})\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': flatten_dataset_train,\n",
    "    'validation': flatten_dataset_valid,\n",
    "    'test': flatten_dataset_test\n",
    "})\n",
    "\n",
    "# Change to conversational manner\n",
    "dataset = dataset.map(convert_to_conversation)\n",
    "\n",
    "# Tokenize dataset\n",
    "dataset = dataset.map(tokenize_function)\n",
    "\n",
    "# Filter conversation longer than tok`en limit\n",
    "dataset = dataset.filter(is_shorter_than_max_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxT8aP-Zps6Y"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DepWFDgOqfB1"
   },
   "outputs": [],
   "source": [
    "class FP32Output(nn.Sequential):\n",
    "    def __init__(self, model: nn.Sequential):\n",
    "        super().__init__(model)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor):\n",
    "        return super().forward(tensor).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5l4p9sQfq0vV"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True, device_map='auto', use_cache=False)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = prepare_model_for_kbit_training(model) # Freeze the weight of the model and some floating point changes.\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.lm_head = FP32Output(model.lm_head) # Change to fp32 for more stable back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwgQh_ZxykXi"
   },
   "outputs": [],
   "source": [
    "# LORA config\n",
    "config = LoraConfig(\n",
    "    r=16, #attention heads\n",
    "    lora_alpha=32, #alpha scaling\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxyMevltzguD"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_dir\",\n",
    "    per_device_train_batch_size=25,\n",
    "    gradient_accumulation_steps=40,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    warmup_steps=50,\n",
    "    fp16=True,\n",
    "    weight_decay=1e-3,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlB26iZlyC7i"
   },
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "def talk_with_llm(chat: str) -> str:\n",
    "    # Encode and move tensor into cuda if applicable.\n",
    "    conversation_history.append(chat)\n",
    "    conversation_history.append(\"\")\n",
    "    conversation = format_conversation(conversation_history)\n",
    "    \n",
    "    encoded_input = tokenizer(conversation, return_tensors='pt')\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "\n",
    "    output = model.generate(**encoded_input, max_new_tokens=256)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    response = response[len(conversation):]\n",
    "    \n",
    "    conversation_history.pop()\n",
    "    conversation_history.append(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cG8eVH0EyC7j"
   },
   "outputs": [],
   "source": [
    "talk_with_llm(\"Yo what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk_with_llm(\"Good, good. Can you help me pick up my kids today? I'll have to run to a doctor's appointment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk_with_llm(\"They get out of school at 3:00 pm.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfZ0bFhqyC7k"
   },
   "source": [
    "## Push the model to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cH4Tx5IsyC7l"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\"danjie/Chadgpt-Llama2-7b-conversation\", commit_message=\"first draft\", private=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzO_Ga6NyC7n"
   },
   "source": [
    "## Load the model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PBWCO-cyC7o"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"danjie/Chadgpt-Llama2-7b-conversation\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
